{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EGRYX_knEtUZ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision.datasets import FashionMNIST\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, random_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aBcnAe8TeMSC"
      },
      "outputs": [],
      "source": [
        "from torch.utils import data\n",
        "def get_default_device():\n",
        "  if torch.cuda.is_available():\n",
        "    return torch.device('cuda')\n",
        "  else:\n",
        "    return torch.device('cpu')\n",
        "\n",
        "  \n",
        "def to_device(data, device):\n",
        "  if isinstance(data, (list, tuple)):\n",
        "    return [to_device(x, device) for x in data]\n",
        "  else:\n",
        "    return data.to(device, non_blocking=True)\n",
        "\n",
        "\n",
        "class DeviceDataLoader():\n",
        "  def __init__(self, dl, device):\n",
        "    self.dl = dl\n",
        "    self.device = device\n",
        "  \n",
        "  def __iter__(self):\n",
        "    for b in self.dl:\n",
        "      yield to_device(b, self.device)\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.dl)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-eyLnBVKEUyv"
      },
      "outputs": [],
      "source": [
        "# # extend nn.Module for further functionality\n",
        "class FashionMnistModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_classes):\n",
        "      super().__init__()\n",
        "      self.linear1 = nn.Linear(input_size, hidden_size)\n",
        "      self.linear2 = nn.Linear(hidden_size, output_classes)\n",
        "        \n",
        "    def forward(self, xb):\n",
        "      xb = xb.view(xb.size(0), -1)\n",
        "      out = self.linear1(xb)\n",
        "      out = F.relu(out)\n",
        "      out = self.linear2(out)\n",
        "      return out\n",
        "    \n",
        "    def training_step(self, batch):\n",
        "      images, labels = batch \n",
        "      out = self(images)                  # Generate predictions\n",
        "      loss = F.cross_entropy(out, labels) # Calculate loss(F.cross_entropy has softmax internally)\n",
        "      return loss\n",
        "    \n",
        "    def validation_step(self, batch):\n",
        "      images, labels = batch \n",
        "      out = self(images)                    # Generate predictions\n",
        "      loss = F.cross_entropy(out, labels)   # Calculate loss(F.cross_entropy has softmax internally)\n",
        "      acc = accuracy(out, labels)           # Calculate accuracy\n",
        "      return {'val_loss': loss, 'val_acc': acc}\n",
        "        \n",
        "    def validation_epoch_end(self, outputs):\n",
        "      batch_losses = [x['val_loss'] for x in outputs]\n",
        "      epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
        "      batch_accs = [x['val_acc'] for x in outputs]\n",
        "      epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n",
        "      return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
        "    \n",
        "    def epoch_end(self, epoch, result):\n",
        "      print(\"Epoch [{}], last_lr: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n",
        "          epoch, result['lr'][-1], result['val_loss'], result['val_acc']))\n",
        "\n",
        "\n",
        "def fit_one_cycle(epochs, max_lr, model, train_loader, \n",
        "                  val_loader, weight_decay=0, grad_clip=None, opt_func=torch.optim.SGD):\n",
        "  torch.cuda.empty_cache()\n",
        "  history = [] # for recording epoch result\n",
        "\n",
        "  optimizer = opt_func(model.parameters(), max_lr, weight_decay=weight_decay)\n",
        "  schedule = torch.optim.lr_scheduler.OneCycleLR(\n",
        "      optimizer, max_lr, epochs=epochs, steps_per_epoch=len(train_loader))\n",
        "  for epoch in range(epochs):\n",
        "      \n",
        "      model.train()\n",
        "      train_loss = []\n",
        "      lr = []\n",
        "      # Training\n",
        "      for batch in train_loader:\n",
        "          loss = model.training_step(batch)\n",
        "          train_loss.append(loss)\n",
        "          loss.backward()\n",
        "\n",
        "          # gradient clipping\n",
        "          if grad_clip:\n",
        "            nn.utils.clip_grad_value_(model.parameters(), grad_clip)\n",
        "\n",
        "\n",
        "          optimizer.step()\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          lr.append(get_lr(optimizer))\n",
        "          schedule.step()\n",
        "      \n",
        "      # Validation\n",
        "      result = evaluate(model, val_loader)\n",
        "      result['train_loss'] = torch.stack(train_loss).mean().item()\n",
        "      result['lr']  = lr\n",
        "      model.epoch_end(epoch, result)\n",
        "      history.append(result)\n",
        "\n",
        "  return history\n",
        "\n",
        "# plot\n",
        "def accuracy_vs_epoch(history):\n",
        "  accuracies = [result['val_acc'] for result in history]\n",
        "  plt.plot(accuracies, '-x')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.ylabel('accuracy')\n",
        "  plt.title('Accuracy vs. No. of epochs')\n",
        "\n",
        "\n",
        "def losses_vs_epoch(history):\n",
        "  train_losses = [result.get('train_loss') for result in history]\n",
        "  val_losses = [result['val_loss'] for result in history]\n",
        "  plt.plot(train_losses, '-bx')\n",
        "  plt.plot(val_losses, '-ro')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.legend(['Training', 'Validation'])\n",
        "  plt.title('Loss vs Epoch')\n",
        "\n",
        "\n",
        "def plot_lr(history):\n",
        "  learning_rate = np.concatenate([result.get('lr', []) for result in history])\n",
        "  plt.plot(learning_rate)\n",
        "  plt.xlabel('Batch')\n",
        "  plt.ylabel('Learning Rate')\n",
        "  plt.title('Learning Rate vs Batch')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ad7HuIW-5r98"
      },
      "outputs": [],
      "source": [
        "# # data preparation\n",
        "dataset = FashionMNIST(\n",
        "    root='./data', download=True, transform=transforms.ToTensor())\n",
        "\n",
        "for_train, for_validation = random_split(dataset, [50000, 10000])\n",
        "# print(len(for_train), len(for_test))\n",
        "batch_size = 128\n",
        "train_loader = DataLoader(\n",
        "    for_train, batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
        "val_loader = DataLoader(\n",
        "    for_validation, batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
        "in_size = 28*28\n",
        "out_classes = 10\n",
        "\n",
        "# # define evaluation fuction\n",
        "@torch.no_grad()\n",
        "def evaluate(model, val_loader):\n",
        "  model.eval()\n",
        "  outputs = [model.validation_step(batch) for batch in val_loader]\n",
        "  return model.validation_epoch_end(outputs)\n",
        "\n",
        "def get_lr(optimizer):\n",
        "  for param_group in optimizer.param_groups:\n",
        "    return param_group['lr']\n",
        "    \n",
        "# # inspect accuracy\n",
        "def accuracy(outputs, labels):\n",
        "  _, preds = torch.max(outputs, dim=1)\n",
        "  return torch.tensor(torch.sum(preds == labels).item() / len(preds))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eMJpg_eU9XJc"
      },
      "outputs": [],
      "source": [
        "# loop data through model\n",
        "device = get_default_device()\n",
        "original_model = FashionMnistModel(input_size=in_size, hidden_size=64, output_classes=out_classes)\n",
        "model = to_device(original_model, device)\n",
        "train_dl = DeviceDataLoader(train_loader, device)\n",
        "valid_dl = DeviceDataLoader(val_loader, device)\n",
        "epochs = 10\n",
        "max_lr = 0.01\n",
        "grad_clip = 0.1\n",
        "weight_decay = 0.0001\n",
        "opt_func = torch.optim.Adam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h8Y-AW_f9NHb"
      },
      "outputs": [],
      "source": [
        "# start to train here!!!\n",
        "%%time\n",
        "history = [evaluate(model, valid_dl)]\n",
        "history += fit_one_cycle(epochs, max_lr, model, train_dl, valid_dl,\n",
        "                         weight_decay, grad_clip, opt_func)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B--XmWQD6wDr"
      },
      "outputs": [],
      "source": [
        "# plot\n",
        "def accuracy_vs_epoch(history):\n",
        "  accuracies = [result['val_acc'] for result in history]\n",
        "  plt.plot(accuracies, '-x')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.ylabel('accuracy')\n",
        "  plt.title('Accuracy vs. No. of epochs')\n",
        "\n",
        "\n",
        "def losses_vs_epoch(history):\n",
        "  train_losses = [result.get('train_loss') for result in history]\n",
        "  val_losses = [result['val_loss'] for result in history]\n",
        "  plt.plot(train_losses, '-bx')\n",
        "  plt.plot(val_losses, '-ro')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.legend(['Training', 'Validation'])\n",
        "  plt.title('Loss vs Epoch')\n",
        "\n",
        "\n",
        "def plot_lr(history):\n",
        "  learning_rate = np.concatenate([result.get('lr', []) for result in history])\n",
        "  plt.plot(learning_rate)\n",
        "  plt.xlabel('Batch')\n",
        "  plt.ylabel('Learning Rate')\n",
        "  plt.title('Learning Rate vs Batch')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PfIjzjOtSJtj"
      },
      "outputs": [],
      "source": [
        "accuracy_vs_epoch(history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jlDX6JkcSHU0"
      },
      "outputs": [],
      "source": [
        "losses_vs_epoch(history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ea7oLY4sURhf"
      },
      "outputs": [],
      "source": [
        "plot_lr(history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F3uocd77_ned"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), 'FashionMNIST-ReLU_modified_GPU.pth')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "FashionMNIST-ReLU_modified_GPU.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.8.9 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.9"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
